# -*- coding: utf-8 -*-
"""TalkingHead-Sad.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Snum6xxVk724P0iMXK7beEkXySJIx3Ib
"""

!pip install pydub
!pip install bark encodec torchaudio transformers fairseq audiolm_pytorch

# !pip install pydub
# !pip install bark encodec torchaudio transformers fairseq audiolm_pytorch

import os

# Create the 'results' folder
if not os.path.exists('results'):
    os.makedirs('results')

# Create the 'sourceimage' folder
if not os.path.exists('sourceimage'):
    os.makedirs('sourceimage')

print("Folders 'results' and 'sourceimage' created successfully.")

"""# **extract the face and the voice**"""

import cv2
import dlib
import os
from pydub import AudioSegment

def extract_audio_from_video(video_path, output_audio_path):
    audio = AudioSegment.from_file(video_path, format="mp4")
    audio.export(output_audio_path, format="wav")
    return output_audio_path

def extract_head_from_video(video_path, horizontal_padding=50, vertical_padding=100):
    cap = cv2.VideoCapture(video_path)
    detector = dlib.get_frontal_face_detector()

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = detector(gray)

        if faces:
            for face in faces:
                x, y, w, h = face.left(), face.top(), face.width(), face.height()

                x_new = max(0, int(x - horizontal_padding))
                y_new = max(0, int(y - vertical_padding))
                w_new = min(frame.shape[1] - x_new, int(w + 2 * horizontal_padding))
                h_new = min(frame.shape[0] - y_new, int(h + vertical_padding * 1.5))

                head_img = frame[y_new:y_new+h_new, x_new:x_new+w_new]
                head_image_path = "extracted_head_with_padding.jpg"
                cv2.imwrite(head_image_path, head_img)
                cap.release()
                return head_image_path

    cap.release()
    return None

# Paths
video_path = "/content/source..mp4"
audio_path = "extracted_audio.wav"

# Extract audio and head from video
extract_audio_from_video(video_path, audio_path)

head_image_path = extract_head_from_video(video_path, horizontal_padding=100, vertical_padding=200)

print(f"Extracted head image saved to: {head_image_path}")

"""# **create acartoon-like avatar**"""

# import torch
# from PIL import Image
# from io import BytesIO

# # Check if CUDA is available and set the device accordingly
# device = "cuda" if torch.cuda.is_available() else "cpu"

# # Load the AnimeGAN2 model
# model = torch.hub.load("bryandlee/animegan2-pytorch:main", "generator", device=device).eval()
# face2paint = torch.hub.load("bryandlee/animegan2-pytorch:main", "face2paint", device=device)

# def generate_anime_image(input_image_path, output_image_path):
#     # Load and process the input image
#     im_in = Image.open(input_image_path).convert("RGB")

#     # Generate the anime version of the image
#     im_out = face2paint(model, im_in, side_by_side=False)

#     # Save the output image
#     im_out.save(output_image_path, format="png")
#     print(f"Anime version saved at {output_image_path}")

# # Example usage
# input_image_path = "/content/extracted_head_with_padding.jpg"  # Replace with your input image path
# output_image_path = "/content/anime_image.png"  # Replace with your desired output image path

# generate_anime_image(input_image_path, output_image_path)

"""# **setup the env**"""

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/serp-ai/bark-with-voice-clone
# %cd bark-with-voice-clone/
!pip install git+https://github.com/suno-ai/bark.git

from bark.generation import load_codec_model, generate_text_semantic
from encodec.utils import convert_audio

import torchaudio
import torch
# device = 'cpu'
device = 'cuda' # or 'cpu'
model = load_codec_model(use_gpu=True if device == 'cuda' else False)

from hubert.hubert_manager import HuBERTManager
hubert_manager = HuBERTManager()
hubert_manager.make_sure_hubert_installed()
hubert_manager.make_sure_tokenizer_installed()

from hubert.pre_kmeans_hubert import CustomHubert
from hubert.customtokenizer import CustomTokenizer

# Load the HuBERT model
hubert_model = CustomHubert(checkpoint_path='data/models/hubert/hubert.pt').to(device)

# Load the CustomTokenizer model
tokenizer = CustomTokenizer.load_from_checkpoint('data/models/hubert/tokenizer.pth').to(device)  # Automatically uses the right layers

from bark.api import generate_audio
from transformers import BertTokenizer
from bark.generation import SAMPLE_RATE, preload_models, codec_decode, generate_coarse, generate_fine, generate_text_semantic

semantic_path = "/content/bark-with-voice-clone/semantic_output/pytorch_model.bin"
coarse_path = "/content/bark-with-voice-clone/coarse_output/pytorch_model.bin"
fine_path = "/content/bark-with-voice-clone/fine_output/pytorch_model.bin"

preload_models(
    text_use_gpu=True,
    text_use_small=False,
    text_model_path=semantic_path,
    coarse_use_gpu=True,
    coarse_use_small=False,
    coarse_model_path=coarse_path,
    fine_use_gpu=True,
    fine_use_small=False,
    fine_model_path=fine_path,
    codec_use_gpu=True,
    force_reload=False,
    path="models"
)

"""# **add the voice that u wanna clone**"""

audio_filepath = "/content/extracted_audio.wav" # the audio you want to clone (under 13 seconds)

voice_name = 'output' # whatever you want the name of the voice to be
output_path =   "/content/" + voice_name + '.npz'

wav, sr = torchaudio.load(audio_filepath)
wav = convert_audio(wav, sr, model.sample_rate, model.channels)
wav = wav.to(device)

semantic_vectors = hubert_model.forward(wav, input_sample_hz=model.sample_rate)
semantic_tokens = tokenizer.get_token(semantic_vectors)
# Extract discrete codes from EnCodec
with torch.no_grad():
    encoded_frames = model.encode(wav.unsqueeze(0))
codes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1).squeeze()  # [n_q, T]

# move codes to cpu
codes = codes.cpu().numpy()
# move semantic tokens to cpu
semantic_tokens = semantic_tokens.cpu().numpy()

import numpy as np

# .npz file will be saved to output_path. The output_path will be used later as history prompt when clone your voice.
np.savez(output_path, fine_prompt=codes, coarse_prompt=codes[:2, :], semantic_prompt=semantic_tokens)

# Enter your prompt and speaker here
text_prompt = "hello this is not my voice i am not real , but i can change the world with y movies "

audio_array = generate_audio(text_prompt, history_prompt=output_path, text_temp=0.7, waveform_temp=0.7)

# generation with more control
x_semantic = generate_text_semantic(
    text_prompt,
    history_prompt=output_path,
    temp=0.7,
    top_k=50,
    top_p=0.95,
)

x_coarse_gen = generate_coarse(
    x_semantic,
    history_prompt=output_path,
    temp=0.7,
    top_k=50,
    top_p=0.95,
)
x_fine_gen = generate_fine(
    x_coarse_gen,
    history_prompt=output_path,
    temp=0.5,
)
audio_array = codec_decode(x_fine_gen)

from IPython.display import Audio
# play audio
Audio(audio_array, rate=SAMPLE_RATE)

from scipy.io.wavfile import write as write_wav
# save audio
filepath = "/content/audio.wav" # change this to your desired output path
write_wav(filepath, SAMPLE_RATE, audio_array)

"""# **talking avatar**"""



!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader

# Commented out IPython magic to ensure Python compatibility.
!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2
!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.9 1
!sudo apt install python3.8

!sudo apt-get install python3.8-distutils

!python --version

!apt-get update

!apt install software-properties-common

!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel

!apt-get install python3-pip

print('Git clone project and install requirements...')
!git clone https://github.com/Winfredy/SadTalker &> /dev/null
# %cd SadTalker
!export PYTHONPATH=/content/SadTalker:$PYTHONPATH
!python3.8 -m pip install torch==1.12.1+cu113 torchvision==0.13.1+cu113 torchaudio==0.12.1 --extra-index-url https://download.pytorch.org/whl/cu113
!apt update
!apt install ffmpeg &> /dev/null
!python3.8 -m pip install -r requirements.txt

print('Download pre-trained models...')
!rm -rf checkpoints
!bash scripts/download_models.sh

import ipywidgets as widgets
import glob
import matplotlib.pyplot as plt
import os

# Fetch list of images
img_list = glob.glob('/content/*.jpg')
img_list.sort()
img_list = [os.path.basename(item).split('.')[0] for item in img_list]

# Handle case where no images are found
if not img_list:
    print("No images found in the directory.")
else:
    # Ensure default value exists in the img_list
    default_value = 'full3' if 'full3' in img_list else img_list[0]
    default_head_name = widgets.Dropdown(options=img_list, value=default_value)

    def on_change(change):
        if change['type'] == 'change' and change['name'] == 'value':
            plt.imshow(plt.imread(f'/content/{default_head_name.value}.jpg'))
            plt.axis('off')
            plt.show()

    default_head_name.observe(on_change)

    display(default_head_name)
    plt.imshow(plt.imread(f'/content/{default_head_name.value}.jpg'))
    plt.axis('off')
    plt.show()

# selected audio from exmaple/driven_audio
img = '/content/{}.jpg'.format(default_head_name.value)
print(img)
!python3.8 inference.py --driven_audio /content/extracted_audio.wav \
           --source_image {img} \
           --result_dir /content/results --still --preprocess full --enhancer gfpgan

# visualize code from makeittalk
from IPython.display import HTML
from base64 import b64encode
import os, sys

# get the last from results

results = sorted(os.listdir('/content/'))

mp4_name = glob.glob('/content/results/*.mp4')[0]

mp4 = open('{}'.format(mp4_name),'rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

print('Display animation: {}'.format(mp4_name), file=sys.stderr)
display(HTML("""
  <video width=256 controls>
        <source src="%s" type="video/mp4">
  </video>
  """ % data_url))